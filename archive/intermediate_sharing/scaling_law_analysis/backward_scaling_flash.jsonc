{
  // ============================================================================
  // Backward Scaling with Flash Attention Example
  // ============================================================================
  // This example shows memory savings with Flash Attention enabled
  
  "architecture": {
    "hidden_size": 4096,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "num_attention_heads": 32,
    "vocab_size": 50000,
    "max_position_embeddings": 2048,
    "tie_word_embeddings": false
  },
  
  "training_gear": {
    "gpu_type": "H100",
    "num_gpus": 8,
    "available_hours": 720,
    "dtype": "bfloat16"
  },
  
  "training_efficiency": {
    "expected_mfu": 0.45,
    "batch_size": 1,
    "gradient_accumulation_steps": 1,
    
    // ✅ Flash Attention ENABLED
    // Saves: batch_size × num_heads × seq² × 2 bytes per layer
    // For batch=1, heads=32, seq=2048: ~0.5 GB per layer
    // Total savings: ~16 GB for 32-layer model
    "use_flash_attention": true
  },
  
  "dataset_constraints": {
    "dataset_size": 1e12,
    "max_epochs": 100,
    "sequence_length": 2048
  },
  
  "scaling_law": {
    "base": "hoffmann_2022",
    "E": 1.69,
    "A": 406.4,
    "B": 410.7,
    "alpha": 0.34,
    "beta": 0.28
  },
  
  "output_options": {
    "show_breakdown": true,
    "verify_calculations": true
  }
}

