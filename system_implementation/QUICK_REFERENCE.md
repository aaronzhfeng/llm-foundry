# Quick Reference: Three-Phase Implementation

## ğŸ“¦ First: Prepare Data

```bash
pip install tiktoken
cd /root/llm_TII/system_implementation/nanoGPT/data/shakespeare
python prepare.py
```

## ğŸ“Š New: Automatic JSON Logging

All phases now automatically save training logs to JSON files:

```bash
# After training, view your logs
cd phase1_zero1
python visualize_training.py  # Auto-finds latest log
# Creates: run_TIMESTAMP_analysis.png

# Or specify a log file
python visualize_training.py out/run_20251031_173650.json
```

**Features:**
- âœ… One JSON file per run with timestamp
- âœ… Complete training history (iterations, losses, times, MFU)
- âœ… Eval steps and checkpoints tracked
- âœ… Easy analysis with Python/pandas
- âœ… Automatic visualization script included

See `LOGGING_GUIDE.md` for full documentation.

## ğŸš€ Fast Testing Commands

### Phase 1: ZeRO-1 (Memory Optimization)
```bash
cd /root/llm_TII/system_implementation/phase1_zero1
torchrun --standalone --nproc_per_node=4 train.py --dataset=shakespeare --use_zero1=True --max_iters=100 --compile=False
```
**Expected**: ~50% memory reduction, "Using ZeRO-1 optimizer state sharding" in logs

---

### Phase 2: Triton (Speed Optimization)
```bash
# First install Triton
pip install triton

cd /root/llm_TII/system_implementation/phase2_triton
python train.py --dataset=shakespeare --max_iters=100 --compile=False
```
**Expected**: ~5-10% speed improvement, "using Triton-accelerated LayerNorm" in logs

---

### Phase 3: FSDP (Maximum Memory Savings)
```bash
cd /root/llm_TII/system_implementation/phase3_fsdp
torchrun --standalone --nproc_per_node=4 train.py --dataset=shakespeare --use_fsdp=True --max_iters=100 --compile=False
```
**Expected**: ~75% memory reduction, "Wrapping model with FSDP..." in logs

---

## ğŸ“Š At-a-Glance Comparison

| Metric | Phase 1 | Phase 2 | Phase 3 |
|--------|---------|---------|---------|
| **Memory Saved** | ğŸŸ¢ 50% | ğŸ”µ ~0% | ğŸŸ¢ğŸŸ¢ 75-88% |
| **Speed Change** | ğŸ”´ -5% | ğŸŸ¢ +5% | ğŸ”´ -10% |
| **Complexity** | ğŸŸ¢ Low | ğŸŸ¡ Medium | ğŸ”´ High |
| **Best For** | Medium models | Speed | Huge models |

---

## ğŸ¯ Decision Tree

```
Do you need more memory?
â”œâ”€ No â†’ Use Phase 2 (Triton) for speed
â””â”€ Yes â†’ How much memory?
    â”œâ”€ 50% reduction enough â†’ Use Phase 1 (ZeRO-1)
    â””â”€ Need maximum savings â†’ Use Phase 3 (FSDP)
```

---

## ğŸ“ Files Changed per Phase

**Phase 1**: Only `train.py` (30 lines)  
**Phase 2**: `model.py` + new `kernels/` package  
**Phase 3**: Only `train.py` (150+ lines)

---

## âš¡ Key Log Messages to Look For

### Phase 1 âœ…
```
Using ZeRO-1 optimizer state sharding
num decayed parameter tensors: ...
```

### Phase 2 âœ…
```
number of parameters: 124.44M
using Triton-accelerated LayerNorm
```

### Phase 3 âœ…
```
Wrapping model with FSDP...
FSDP enabled with min_params=1000000.0, mixed_precision=bfloat16
```

---

## ğŸ”§ Common Flags

| Flag | Phase | Description |
|------|-------|-------------|
| `--use_zero1=True/False` | 1 | Enable/disable ZeRO-1 |
| `--use_fsdp=True/False` | 3 | Enable/disable FSDP |
| `--fsdp_min_num_params=1e6` | 3 | FSDP wrapping threshold |
| `--compile=True/False` | All | torch.compile |
| `--dtype=bfloat16` | All | Mixed precision |

---

## ğŸ“– Full Documentation

- **Phase 1**: `phase1_zero1/IMPLEMENTATION.md`
- **Phase 2**: `phase2_triton/IMPLEMENTATION.md`
- **Phase 3**: `phase3_fsdp/IMPLEMENTATION.md`
- **Summary**: `IMPLEMENTATION_SUMMARY.md`

---

## ğŸ› Quick Troubleshooting

**"Triton not found"**  
â†’ `pip install triton` (Linux + CUDA only)

**FSDP OOM**  
â†’ Reduce `--fsdp_min_num_params=500000`

**ZeRO-1 checkpoint error**  
â†’ Use checkpoint from rank 0 only

**Slow FSDP**  
â†’ Tune `fsdp_min_num_params` (500K-5M range)

---

**Ready to test!** ğŸ‰

